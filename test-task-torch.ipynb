{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496b3711-787d-476c-b901-56ad5bd9d54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bb865d-1d4b-4522-abce-b1505844c686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44fdf93b-30b9-4c3e-bf4c-f251471b8f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2abc2fee-92cf-41e5-bcd6-f842bfc3b18c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    assert len(X.shape) == 4, \"X must be with shape [batch, h, w, c]\"\n",
    "    \n",
    "    X_pad = F.pad(X, (\n",
    "        *(0, 0), \n",
    "        *(pad, pad),\n",
    "        *(pad, pad),\n",
    "        *(0, 0)\n",
    "    ), value=0)\n",
    "\n",
    "    return X_pad\n",
    "\n",
    "\n",
    "\n",
    "class Conv2D:\n",
    "    def __init__(self, prev_channels, next_channels, kernel=3, stride=1, pad=0):\n",
    "        self._cache = None\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.kernel = torch.randn(kernel, kernel, prev_channels, next_channels)\n",
    "        self.b = torch.zeros(next_channels)\n",
    "    \n",
    "    def forward(self, A_prev):\n",
    "        # (m, n_H_prev, n_W_prev, n_C_prev) -> (m, n_H, n_W, n_C) by (k, k, n_C_prev, n_C)\n",
    "\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        k, k, n_C_prev, n_C = self.kernel.shape\n",
    "\n",
    "        n_H = (n_H_prev + 2*self.pad - k) // self.stride + 1\n",
    "        n_W = (n_W_prev + 2*self.pad - k) // self.stride + 1\n",
    "\n",
    "        output = torch.zeros((m, n_H, n_W, n_C))\n",
    "        A_prev_pad = zero_pad(A_prev, pad=self.pad)\n",
    "        \n",
    "        for i in range(m):\n",
    "            a_prev_pad = A_prev_pad[i]\n",
    "\n",
    "            for h in range(n_H):\n",
    "                h_start = h * self.stride \n",
    "                h_end = h_start + k\n",
    "\n",
    "                for w in range(n_W):\n",
    "                    w_start = w * self.stride\n",
    "                    w_end = w_start + k\n",
    "\n",
    "                    for c in range(n_C):\n",
    "                        a_slice = a_prev_pad[h_start:h_end, w_start:w_end, :]\n",
    "                        current_kernel = self.kernel[:, :, :, c]\n",
    "                        current_bias = self.b[c]\n",
    "                       \n",
    "                        output[i, h, w, c] = torch.sum(a_slice * current_kernel) + current_bias\n",
    "        \n",
    "        self.cache = A_prev\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A_prev = self.cache\n",
    "        \n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        k, k, n_C_prev, n_C = self.kernel.shape\n",
    "        m, n_H, n_W, n_C = dZ.shape \n",
    "        \n",
    "        dA_prev = torch.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "        dW = torch.zeros((k, k, n_C_prev, n_C))\n",
    "        db = torch.zeros(n_C)\n",
    "        \n",
    "        A_prev_pad = zero_pad(A_prev, self.pad)\n",
    "        dA_prev_pad = zero_pad(dA_prev, self.pad)\n",
    "        \n",
    "        for i in range(m):\n",
    "            a_prev_pad = A_prev_pad[i]\n",
    "            da_prev_pad = dA_prev_pad[i]\n",
    "            \n",
    "            for h in range(n_H):\n",
    "                h_start = h * self.stride \n",
    "                h_end = h_start + k\n",
    "                \n",
    "                for w in range(n_W):\n",
    "                    w_start = w * self.stride\n",
    "                    w_end = w_start + k\n",
    "                    \n",
    "                    for c in range(n_C):\n",
    "                        a_slice = a_prev_pad[h_start:h_end, w_start:w_end, :]\n",
    "                        \n",
    "                        da_prev_pad[h_start:h_end, w_start:w_end, :] += self.kernel[:, :, :, c] * dZ[i, h, w, c]\n",
    "                        dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
    "                        db[c] += dZ[i, h, w, c]\n",
    "            if self.pad == 0:\n",
    "                dA_prev[i, :, :, :] = da_prev_pad[:, :, :]\n",
    "            else:\n",
    "                dA_prev[i, :, :, :] = da_prev_pad[self.pad:-self.pad, self.pad:-self.pad, :]\n",
    "            \n",
    "        assert dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "            \n",
    "        return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04ae3a52-e9b3-40d9-9684-8f698708d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(X, stride=2, pad=0):\n",
    "    assert len(X.shape) == 4, \"X should be with shape [batch, h, w, c]\"\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, c = X.shape\n",
    "    \n",
    "    n_H = stride * (n_H_prev - 1) + 1\n",
    "    n_W = stride * (n_W_prev - 1) + 1\n",
    "    \n",
    "    upsampled = torch.zeros((m, n_H, n_W, c))\n",
    "    \n",
    "    for h in range(n_H_prev):\n",
    "        for w in range(n_W_prev):\n",
    "            upsampled[:, h*stride, w*stride, :] = X[:, h, w, :]\n",
    "    \n",
    "    upsampled_pad = zero_pad(upsampled, pad=pad)\n",
    "    \n",
    "    return upsampled_pad\n",
    "\n",
    "\n",
    "def downsample(X, stride, pad): \n",
    "    assert stride >= 0\n",
    "    assert pad >= 0\n",
    "    \n",
    "    if pad == 0:\n",
    "        res = X[:, ::stride, ::stride, :]\n",
    "    else:\n",
    "        res = X[:, pad:-pad:stride, pad:-pad:stride, :]\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "class Conv2DTranspose:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=0):\n",
    "        self.cache = None \n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.conv = Conv2D(in_channels, out_channels, kernel=kernel_size, stride=1, pad=0)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape \n",
    "        k, k, n_C_prev, n_C = self.kernel.shape\n",
    "        \n",
    "        n_H = self.stride * (n_H_prev - 1) + k - 2 * self.padding\n",
    "        n_W = self.stride * (n_W_prev - 1) + k - 2 * self.padding\n",
    "        \n",
    "        implicit_padding = k - self.padding - 1\n",
    "        assert implicit_padding >= 0\n",
    "        \n",
    "        A_prev_upsampled = upsample(A_prev, stride=self.stride, pad=implicit_padding)\n",
    "        out = self.conv.forward(A_prev_upsampled)\n",
    "        \n",
    "        assert out.shape == (m, n_H, n_W, n_C)\n",
    "        \n",
    "        self.cache = A_prev\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A_prev = self.cache \n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        k, k, n_C_prev, n_C = self.kernel.shape\n",
    "        m, n_H, n_W, n_C = dZ.shape\n",
    "        \n",
    "        dA_upsampled, dW, db = self.conv.backward(dZ)\n",
    "        \n",
    "        implicit_padding = k - self.padding - 1\n",
    "        dA = downsample(dA_upsampled, self.stride, pad=implicit_padding)\n",
    "        \n",
    "        return dA, dW, db\n",
    "    \n",
    "    @property \n",
    "    def kernel(self):\n",
    "        return self.conv.kernel\n",
    "    \n",
    "    @kernel.setter\n",
    "    def kernel(self, weights):\n",
    "        self.conv.kernel = weights\n",
    "        \n",
    "    @property \n",
    "    def b(self):\n",
    "        return self.conv.b\n",
    "    \n",
    "    @kernel.setter\n",
    "    def b(self, biases):\n",
    "        self.conv.b = biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1890ad71-182a-4582-8b3e-2e3113bbfbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conv2DTransposeV2:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, output_padding=1):\n",
    "        self.cache = None\n",
    "        self.stride = stride\n",
    "        self.pad = padding \n",
    "        self.output_pad = output_padding\n",
    "        self.kernel = torch.rand(kernel_size, kernel_size, in_channels, out_channels)\n",
    "        self.b = torch.zeros(out_channels)\n",
    "        \n",
    "        self.kernel.requires_grad = True \n",
    "        self.b.requires_grad = True\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        k, k, n_C_prev, n_C = self.kernel.shape\n",
    "        \n",
    "        n_H = self.stride * (n_H_prev - 1) + k - 2 * self.pad + self.output_pad\n",
    "        n_W = self.stride * (n_W_prev - 1) + k - 2 * self.pad + self.output_pad\n",
    "        \n",
    "        output_pad = torch.zeros((m, n_H - self.output_pad + 2*self.pad, n_W - self.output_pad + 2*self.pad, n_C))\n",
    "        \n",
    "        for c_out in range(n_C): # by kernels/out_channels\n",
    "            \n",
    "            for h in range(n_H_prev):\n",
    "                h_start = h * self.stride \n",
    "                h_end = h_start + k\n",
    "\n",
    "                for w in range(n_W_prev):\n",
    "                    w_start = w * self.stride\n",
    "                    w_end = w_start + k \n",
    "                    \n",
    "                    output_pad[:, h_start:h_end, w_start:w_end, c_out] += (A_prev[:, h, w, :] * self.kernel[:, :, :, c_out]).sum(axis=-1)\n",
    "            \n",
    "            output_pad[:, :, :, c_out] += self.b[c_out]\n",
    "                \n",
    "        output = output_pad[:, self.output_pad:, self.output_pad:, :]\n",
    "        \n",
    "        return output    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        output = self.cache \n",
    "        \n",
    "        grads = output.backward(gradient=dZ)\n",
    "        print(grads.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ae86a-626c-4e8c-8aca-f9f0721fc796",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e258e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test zero_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da845423-11ef-46e9-aabf-9684b311e9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 40, 40, 3]) torch.Size([30, 42, 42, 3])\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "m = 30\n",
    "h = 40\n",
    "w = 40\n",
    "c = 3\n",
    "pad = 1\n",
    "\n",
    "X_batch = torch.rand(m, h, w, c)\n",
    "shape1 = X_batch.shape\n",
    "X_pad = zero_pad(X_batch, pad=pad)\n",
    "shape2 = X_pad.shape\n",
    "\n",
    "print(shape1, shape2)\n",
    "print(\n",
    "    shape1[0] == shape2[0], \n",
    "    shape1[1]+pad*2 == shape2[1], \n",
    "    shape1[2]+2*pad == shape2[2], \n",
    "    shape1[3] == shape2[3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65249e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b = np.random.rand(1, 1, 1)\n",
    "b = 1\n",
    "b = np.array(b)\n",
    "b.shape == (1, 1, 1) or b.shape == ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c99ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conv2D forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6361df3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 33, 33, 16])\n"
     ]
    }
   ],
   "source": [
    "m = 16\n",
    "n_H_prev = 33\n",
    "n_W_prev = 33\n",
    "n_C_prev = 8\n",
    "n_C = 16\n",
    "\n",
    "kernel = 3\n",
    "pad = 1\n",
    "stride = 1\n",
    "\n",
    "\n",
    "X_batch = torch.rand(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "conv2d = Conv2D(n_C_prev, n_C, kernel=kernel, pad=pad, stride=stride)\n",
    "out = conv2d.forward(X_batch)\n",
    "print(out.shape)\n",
    "\n",
    "del m \n",
    "del n_H_prev\n",
    "del n_W_prev\n",
    "del n_C_prev\n",
    "del n_C\n",
    "del kernel\n",
    "del pad \n",
    "del stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c95c9f-5f4f-49c5-8b93-a4b9d7279ad7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Comparsion Conv2D forward with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4628ee0-0aed-4e12-9cbc-0a59a792a101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.4674e-13)\n",
      "\u001b[92m\u001b[1mOK\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def compare_conv(\n",
    "            m, \n",
    "            n_H_prev, \n",
    "            n_W_prev, \n",
    "            n_C_prev, \n",
    "            n_C, \n",
    "            kernel,\n",
    "            pad, \n",
    "            stride\n",
    "        ):\n",
    "    \n",
    "    input_data = torch.rand(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    \n",
    "    # my realization \n",
    "    my_conv = Conv2D(n_C_prev, n_C, kernel=kernel, stride=stride, pad=pad)\n",
    "    my_out = my_conv.forward(input_data)\n",
    "    \n",
    "    # torch_realizatoin \n",
    "    torch_input = torch.permute(input_data, [0, 3, 1, 2]) # (m, n_C_prev, n_H_prev, n_W_prev)\n",
    "    torch_weights = torch.permute(my_conv.kernel.clone(), [3, 2, 0, 1]) # [k, k, channels, filters] -> [filters, channels, k, k]\n",
    "    torch_biases = my_conv.b.clone()\n",
    "    torch_out_ = F.conv2d(torch_input, torch_weights, bias=torch_biases, padding=pad, stride=stride)\n",
    "    \n",
    "    torch_out = torch.permute(torch_out_, [0, 2, 3, 1]).numpy() \n",
    "    \n",
    "    mse = np.power(my_out - torch_out, 2).mean()\n",
    "    \n",
    "    return mse\n",
    "\n",
    "res = compare_conv(\n",
    "    m = 16,\n",
    "    n_H_prev = 32,\n",
    "    n_W_prev = 32,\n",
    "    n_C_prev = 8,\n",
    "    n_C = 16,\n",
    "\n",
    "    kernel = 3,\n",
    "    pad = 1,\n",
    "    stride = 1,\n",
    ")\n",
    "print(res)\n",
    "\n",
    "if res < EPS:\n",
    "    print(Color.GREEN + Color.BOLD + \"OK\")\n",
    "else:\n",
    "    print(Color.RED + Color.BOLD + \"Error\")\n",
    "    \n",
    "del res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcb239-de64-4fab-9475-e95e4a62d0e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conv2d backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13ced207-1c2e-4687-b408-7af8899dc328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 32, 8])\n",
      "torch.Size([3, 3, 8, 16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "kernel = 3\n",
    "pad = 1\n",
    "stride = 1\n",
    "\n",
    "m = 16\n",
    "n_H_prev = 32\n",
    "n_W_prev = 32\n",
    "n_C_prev = 8\n",
    "n_C = 16\n",
    "\n",
    "n_H = (n_H_prev + 2*pad - kernel) // stride + 1\n",
    "n_W = (n_W_prev + 2*pad - kernel) // stride + 1\n",
    "\n",
    "input_data = torch.rand(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "dZ = torch.rand(m, n_H, n_W, n_C)\n",
    "\n",
    "my_conv = Conv2D(n_C_prev, n_C, kernel=kernel, stride=stride, pad=pad)\n",
    "conv_out = my_conv.forward(input_data)\n",
    "\n",
    "dA, dW, db = my_conv.backward(dZ)\n",
    "print(dA.shape)\n",
    "print(dW.shape)\n",
    "print(db.shape)\n",
    "\n",
    "\n",
    "del kernel\n",
    "del pad\n",
    "del stride\n",
    "\n",
    "del m\n",
    "del n_H_prev\n",
    "del n_W_prev\n",
    "del n_C_prev\n",
    "del n_C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8064d5-1118-43e7-88e7-e0bf57ca65b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Comparsion Conv2D backward with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b04000f-41f0-4dc1-9a00-b0254a2d1225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_conv_back(\n",
    "            kernel,\n",
    "            pad,\n",
    "            stride,\n",
    "            m,\n",
    "            n_H_prev,\n",
    "            n_W_prev,\n",
    "            n_C_prev,\n",
    "            n_C,\n",
    "        ):\n",
    "    n_H = (n_H_prev + 2*pad - kernel) // stride + 1\n",
    "    n_W = (n_W_prev + 2*pad - kernel) // stride + 1\n",
    "    \n",
    "    input_data = torch.rand(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dZ = torch.rand(m, n_H, n_W, n_C)\n",
    "    \n",
    "    weights = torch.rand(kernel, kernel, n_C_prev, n_C)\n",
    "    biases = torch.zeros(n_C)\n",
    "    \n",
    "    # my realization \n",
    "    my_conv = Conv2D(n_C_prev, n_C, kernel=kernel, stride=stride, pad=pad)\n",
    "    my_conv.kernel = weights.clone()\n",
    "    my_conv.b = biases.clone()\n",
    "    \n",
    "    conv_out = my_conv.forward(input_data)\n",
    "\n",
    "    dA, dW, db = my_conv.backward(dZ)\n",
    "    \n",
    "    # torch realization\n",
    "    torch_input = torch.permute(input_data, [0, 3, 1, 2])\n",
    "    torch_dZ = torch.permute(dZ, [0, 3, 1, 2])\n",
    "    torch_weights = torch.permute(weights.detach().clone(), [3, 2, 0, 1])\n",
    "    torch_biases = biases.detach().clone()\n",
    "    \n",
    "    torch_input.requires_grad = True\n",
    "    torch_weights.requires_grad = True\n",
    "    torch_biases.requires_grad = True\n",
    "    torch_dZ.requires_grad = False\n",
    "    \n",
    "    torch_out = F.conv2d(torch_input, torch_weights, bias=torch_biases, padding=pad, stride=stride)\n",
    "    \n",
    "    torch_out.backward(gradient=torch_dZ)\n",
    "    \n",
    "    dA_torch = torch_input.grad\n",
    "    dW_torch = torch_weights.grad\n",
    "    db_torch = torch_biases.grad\n",
    "    \n",
    "    dA_torch = torch.permute(dA_torch, [0, 2, 3, 1]) # [m, channels, h, w] -> [m, h, w, channels]\n",
    "    dW_torch = torch.permute(dW_torch, [2, 3, 1, 0]) # [filters, channels, k, k] -> [k, k, channels, filters] \n",
    "    db_torch = db_torch\n",
    "    assert dA.shape == dA_torch.shape and dW.shape == dW_torch.shape and db.shape == db_torch.shape\n",
    "    \n",
    "    mse = lambda a, b: np.power(a - b, 2).mean()\n",
    "    mse_dA = mse(dA, dA_torch)\n",
    "    mse_dW = mse(dW, dW_torch)\n",
    "    mse_db = mse(db, db_torch)\n",
    "    \n",
    "    return mse_dA, mse_dW, mse_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbf7bd18-cdb1-4402-8cb4-2f0dc7cf1594",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5672e-11)\n",
      "tensor(7.4581e-09)\n",
      "tensor(4.0396e-08)\n",
      "\u001b[92m\u001b[1mOK\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mse_dA, mse_dW, mse_db = compare_conv_back(\n",
    "    kernel = 3,\n",
    "    pad = 1,\n",
    "    stride = 1,\n",
    "\n",
    "    m = 1,\n",
    "    n_H_prev = 32,\n",
    "    n_W_prev = 32,\n",
    "    n_C_prev = 8,\n",
    "    n_C = 16\n",
    ")\n",
    "\n",
    "print(mse_dA)\n",
    "print(mse_dW)\n",
    "print(mse_db)\n",
    "\n",
    "if mse_dA < EPS and mse_dW < EPS and mse_db < EPS:\n",
    "    print(Color.GREEN + Color.BOLD + \"OK\")\n",
    "else:\n",
    "    print(Color.RED + Color.BOLD + \"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e84dfb-97f5-4321-9ab6-a65798a56745",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conv2D Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f9e666f-3e54-418a-84ad-6bc15bce9b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   4.,   0.],\n",
      "        [ 18., 127.,  57.],\n",
      "        [  0.,   8.,   0.]])\n"
     ]
    }
   ],
   "source": [
    "arr = torch.tensor([\n",
    "    [2, 5],\n",
    "    [4, 13]\n",
    "])\n",
    "kernel = torch.tensor([\n",
    "    [3, 1, 2],\n",
    "    [2, 0, 0],\n",
    "    [5, 4, 7]\n",
    "])\n",
    "\n",
    "batch = arr[None, ..., None]\n",
    "weights = kernel[..., None, None]\n",
    "\n",
    "convt = Conv2DTranspose(1, 1, kernel_size=3, stride=2, padding=1)\n",
    "convt.kernel = weights\n",
    "\n",
    "out = convt.forward(batch)\n",
    "print(out.squeeze())\n",
    "\n",
    "\n",
    "del convt\n",
    "del out \n",
    "del weights \n",
    "del batch\n",
    "del kernel \n",
    "del arr\n",
    "\n",
    " # [  0   4   0]\n",
    " # [ 18 127  57]\n",
    " # [  0   8   0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd766b-6491-4c2c-94e1-a8a0fb875c3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Comparsion Conv2DTranspose forward with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c27de602-5f7b-4e65-b92b-551fab0567d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0543)\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "def compare_convt(\n",
    "            m,\n",
    "            n_H_prev,\n",
    "            n_W_prev,\n",
    "            n_C_prev,\n",
    "            n_C,\n",
    "            kernel,\n",
    "            pad,\n",
    "            stride\n",
    "        ):\n",
    "    input_data = torch.rand(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    weights = torch.rand(kernel, kernel, n_C_prev, n_C)\n",
    "    biases = torch.zeros(n_C)\n",
    "    \n",
    "    # my realizatoin \n",
    "    my_convt = Conv2DTranspose(n_C_prev, n_C, kernel_size=kernel, stride=stride, padding=pad)\n",
    "    my_convt.kernel = weights.clone()\n",
    "    my_convt.b = biases.clone()\n",
    "    \n",
    "    my_out = my_convt.forward(input_data)\n",
    "    \n",
    "    # torch realization \n",
    "    torch_input = torch.permute(input_data, [0, 3, 1, 2]) # (m, n_C_prev, n_H_prev, n_W_prev)\n",
    "    torch_weights = torch.permute(weights.clone(), [2, 3, 0, 1]) # [k, k, channels, filters] -> [channels, filters, k, k]\n",
    "    torch_biases = biases.clone()\n",
    "    torch_out_ = F.conv_transpose2d(torch_input, torch_weights, bias=torch_biases, padding=pad, stride=stride)\n",
    "    \n",
    "    torch_out = torch.permute(torch_out_, [0, 2, 3, 1]).numpy() \n",
    "    \n",
    "    mse = torch.pow(my_out - torch_out, 2).mean()\n",
    "    return mse\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "res = compare_convt(\n",
    "    m = 1,\n",
    "    n_H_prev = 2,\n",
    "    n_W_prev = 2,\n",
    "    n_C_prev = 1,\n",
    "    n_C = 1,\n",
    "    kernel = 3,\n",
    "    pad = 0,\n",
    "    stride = 1\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a6cc9-c02d-42e5-8e9d-6788eccdfd95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conv2D Transpose backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b38bb1ad-dfe1-4ac2-8d1b-d2a93742e102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 16, 32])\n",
      "torch.Size([3, 3, 32, 8])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "kernel = 3\n",
    "pad = 1\n",
    "stride = 1\n",
    "\n",
    "m = 4\n",
    "n_H_prev = 16\n",
    "n_W_prev = 16\n",
    "n_C_prev = 32\n",
    "n_C = 8\n",
    "\n",
    "n_H = stride * (n_H_prev - 1) + kernel - 2 * pad\n",
    "n_W = stride * (n_W_prev - 1) + kernel - 2 * pad\n",
    "\n",
    "input_data = torch.rand(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "dZ = torch.rand(m, n_H, n_W, n_C)\n",
    "\n",
    "convt = Conv2DTranspose(n_C_prev, n_C, kernel_size=kernel, stride=stride, padding=pad)\n",
    "fout = convt.forward(input_data)\n",
    "dA, dW, dz = convt.backward(dZ)\n",
    "\n",
    "print(dA.shape)\n",
    "print(dW.shape)\n",
    "print(db.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037df71b-aa28-442c-92b9-b1086b470e03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Comparsion Conv2DTranspose backward with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29f7b97e-2070-4249-8a30-9f42db172a26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "def compare_convt_back(\n",
    "            kernel,\n",
    "            pad,\n",
    "            stride,\n",
    "            m,\n",
    "            n_H_prev,\n",
    "            n_W_prev,\n",
    "            n_C_prev,\n",
    "            n_C,\n",
    "        ):\n",
    "    n_H = stride * (n_H_prev - 1) + kernel - 2 * pad\n",
    "    n_W = stride * (n_W_prev - 1) + kernel - 2 * pad\n",
    "    \n",
    "    input_data = torch.rand(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dZ = torch.ones((m, n_H, n_W, n_C))\n",
    "    \n",
    "    weights = torch.arange(kernel * kernel * n_C_prev * n_C).reshape(kernel, kernel, n_C_prev, n_C).type(torch.float32)\n",
    "    biases = torch.rand(n_C)\n",
    "    \n",
    "    # my realization \n",
    "    convt = Conv2DTranspose(n_C_prev, n_C, kernel_size=kernel, stride=stride, padding=pad)\n",
    "    convt.kernel = weights.clone()\n",
    "    convt.biases = biases.clone()\n",
    "    \n",
    "    my_out = convt.forward(input_data)\n",
    "    dA, dW, db = convt.backward(dZ)\n",
    "    \n",
    "    # torch realization \n",
    "    torch_input = torch.permute(input_data, [0, 3, 1, 2]) # (batch, h, w, in_channels) -> (batch, in_channels, h, w)\n",
    "    torch_dZ = torch.permute(dZ, [0, 3, 1, 2]) # (batch, h, w, out_channels) -> (batch, out_channels, h, w)\n",
    "    torch_weights = torch.permute(weights.clone(), [2, 3, 0, 1]) # (k, k, in_channels, out_channels) -> (in_channels, out_channels, k, k)\n",
    "    torch_biases = biases.clone()\n",
    "    \n",
    "    torch_input.requires_grad = True\n",
    "    torch_weights.requires_grad = True\n",
    "    torch_biases.requires_grad = True\n",
    "    torch_dZ.requires_grad = False\n",
    "    \n",
    "    torch_out = F.conv_transpose2d(torch_input, torch_weights, bias=torch_biases, padding=pad, stride=stride)\n",
    "    torch_out.backward(gradient=torch_dZ)\n",
    "    \n",
    "    dA_torch = torch_input.grad\n",
    "    dW_torch = torch_weights.grad\n",
    "    db_torch = torch_biases.grad\n",
    "    \n",
    "    dA_torch = torch.permute(dA_torch, [0, 2, 3, 1]) # (batch, in_channels, h, w) -> (batch, h, w, in_channels)\n",
    "    dW_torch = torch.permute(dW_torch, [2, 3, 0, 1]) # (in_channels, out_channels, k, k) -> (k, k, in_channels, out_channels) \n",
    "    # db_torch = db_torch.numpy()\n",
    "    \n",
    "    assert dA.shape == dA_torch.shape and dW.shape == dW_torch.shape and db.shape == db_torch.shape\n",
    "    \n",
    "    mse = lambda a, b: torch.pow(a - b, 2).mean()\n",
    "    mse_dA = mse(dA, dA_torch)\n",
    "    mse_dW = mse(dW, dW_torch)\n",
    "    mse_db = mse(db, db_torch)\n",
    "    \n",
    "    return mse_dA, mse_dW, mse_db\n",
    "    \n",
    "    \n",
    "\n",
    "mse_dA, mse_dW, mse_db = compare_convt_back(\n",
    "    m = 1,\n",
    "    n_H_prev = 2,\n",
    "    n_W_prev = 2,\n",
    "    n_C_prev = 1,\n",
    "    n_C = 1,\n",
    "    kernel = 3,\n",
    "    pad = 0,\n",
    "    stride = 1\n",
    ")\n",
    "\n",
    "print(mse_dA)\n",
    "print(mse_dW)\n",
    "print(mse_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ff66d-e056-4948-a1b9-25ee4c8c0063",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conv2DTransposeV2 forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "549428f8-911d-4723-97e0-5043f903861d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 4, 1])\n",
      "tensor([[2., 1., 1., 1.],\n",
      "        [1., 2., 1., 3.],\n",
      "        [1., 1., 2., 1.],\n",
      "        [1., 2., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "input_img = torch.tensor([\n",
    "    [1, 0],\n",
    "    [0, 1]\n",
    "])\n",
    "\n",
    "kernel = torch.tensor([\n",
    "    [1, 0, 2],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0]\n",
    "])\n",
    "\n",
    "batch = input_img[None, ..., None]\n",
    "weights = kernel[..., None, None]\n",
    "biases = np.ones((1))\n",
    "\n",
    "\n",
    "convt2 = Conv2DTransposeV2(1, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "convt2.kernel = weights\n",
    "convt2.b = biases\n",
    "\n",
    "out = convt2.forward(batch)\n",
    "print(out.shape)\n",
    "print(out.squeeze())\n",
    "\n",
    "#  2 1 1 1\n",
    "#  1 2 1 3\n",
    "#  1 1 2 1\n",
    "#  1 2 1 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddaa325-75a8-4bc7-acee-1701ef8ce56b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparsion Conv2DTransposeV2 forward with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35ef92dc-80e3-4a99-910a-2adb4bcd94fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "\u001b[92m\u001b[1mOK\n"
     ]
    }
   ],
   "source": [
    "def compare_convt(\n",
    "            m,\n",
    "            n_H_prev,\n",
    "            n_W_prev,\n",
    "            n_C_prev,\n",
    "            n_C,\n",
    "            kernel,\n",
    "            pad,\n",
    "            stride,\n",
    "            output_pad\n",
    "        ):\n",
    "    input_data = torch.rand(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    weights = torch.rand(kernel, kernel, n_C_prev, n_C)\n",
    "    biases = torch.zeros(n_C)\n",
    "    \n",
    "    # my realizatoin \n",
    "    my_convt = Conv2DTransposeV2(n_C_prev, n_C, kernel_size=kernel, stride=stride, padding=pad, output_padding=output_pad)\n",
    "    my_convt.kernel = weights.clone()\n",
    "    my_convt.b = biases.clone()\n",
    "    \n",
    "    my_out = my_convt.forward(input_data)\n",
    "    \n",
    "    # torch realization \n",
    "    torch_input = torch.permute(input_data, [0, 3, 1, 2]) # (m, n_C_prev, n_H_prev, n_W_prev)\n",
    "    torch_weights = torch.permute(weights.clone(), [2, 3, 0, 1]) # [k, k, channels, filters] -> [channels, filters, k, k]\n",
    "    torch_biases = biases.clone()\n",
    "    torch_out_ = F.conv_transpose2d(torch_input, torch_weights, bias=torch_biases, padding=pad, stride=stride, output_padding=output_pad)\n",
    "    \n",
    "    torch_out = torch.permute(torch_out_, [0, 2, 3, 1]).numpy() \n",
    "    \n",
    "    mse = np.power(my_out - torch_out, 2).mean()\n",
    "    return mse\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "res = compare_convt(\n",
    "    m = 1,\n",
    "    n_H_prev = 2,\n",
    "    n_W_prev = 2,\n",
    "    n_C_prev = 1,\n",
    "    n_C = 1,\n",
    "    kernel = 3,\n",
    "    pad = 1,\n",
    "    stride = 2,\n",
    "    output_pad=1\n",
    ")\n",
    "print(res)\n",
    "\n",
    "if res < EPS:\n",
    "    print(Color.GREEN + Color.BOLD + \"OK\")\n",
    "else:\n",
    "    print(Color.RED + Color.BOLD + \"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e5fe9-a115-47ec-b46b-7543f24b8f14",
   "metadata": {},
   "source": [
    "## Conv2DTranspose backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96561796-493b-4fb6-b15d-13e422bbc946",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 4, 1])\n",
      "tensor([[0.7981, 0.7718, 0.0000, 0.0000],\n",
      "        [0.8100, 1.0559, 0.2843, 0.3398],\n",
      "        [0.0000, 0.5239, 0.7981, 0.7718],\n",
      "        [0.0000, 0.0112, 0.8100, 0.6397]], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_img = torch.tensor([\n",
    "    [1., 0.],\n",
    "    [0., 1.]\n",
    "])\n",
    "\n",
    "# kernel = torch.tensor([\n",
    "#     [1., 0., 2.],\n",
    "#     [0., 1., 0.],\n",
    "#     [1., 0., 0.]\n",
    "# ])\n",
    "\n",
    "batch = input_img[None, ..., None]\n",
    "# weights = kernel[..., None, None]\n",
    "# biases = torch.ones((1))\n",
    "\n",
    "\n",
    "convt2 = Conv2DTransposeV2(1, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "# convt2.kernel = weights\n",
    "# convt2.b = biases\n",
    "\n",
    "out = convt2.forward(batch)\n",
    "print(out.shape)\n",
    "print(out.squeeze())\n",
    "\n",
    "dZ = torch.rand(*out.shape)\n",
    "# convt2.backward(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca4d39d-4060-47d5-a5a4-0fd64e46d580",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "492ff292-26dc-48f4-91fd-e9b54bd524a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 4.4166e+21]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([6, 7])]\n",
    "torch.Tensor(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3501e2-8655-46d7-a2d8-d3bb662ebf93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
